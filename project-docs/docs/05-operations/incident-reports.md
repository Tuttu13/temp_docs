---
sidebar_position: 5
---

# 障害対応記録・改善報告書

## 📌 概要
トラブル対応と再発防止策を記録するドキュメントです。

## 🚨 障害報告テンプレート

### INC-XXX: [障害タイトル]

| 項目 | 内容 |
|------|------|
| **インシデントID** | INC-XXX |
| **発生日時** | YYYY-MM-DD HH:MM:SS |
| **検知日時** | YYYY-MM-DD HH:MM:SS |
| **復旧日時** | YYYY-MM-DD HH:MM:SS |
| **影響時間** | XX時間XX分 |
| **影響範囲** | |
| **影響ユーザー数** | |
| **優先度** | P1/P2/P3/P4 |
| **ステータス** | 発生中/復旧済み/完了 |
| **担当者** | |

**概要**:

**影響**:

**原因**:

**対応内容**:

**再発防止策**:

---

## 📋 障害記録

### INC-001: データベース接続プール枯渇によるサービス停止

| 項目 | 内容 |
|------|------|
| **インシデントID** | INC-001 |
| **発生日時** | 2025-11-09 14:30:00 |
| **検知日時** | 2025-11-09 14:32:00 |
| **復旧日時** | 2025-11-09 15:15:00 |
| **影響時間** | 45分 |
| **影響範囲** | 全サービス |
| **影響ユーザー数** | 約5,000名 |
| **優先度** | P1 (Critical) |
| **ステータス** | 完了 |
| **担当者** | 鈴木一郎、山田花子 |

#### 概要
データベース接続プールが枯渇し、新規リクエストが処理できなくなった。ユーザーは500エラーを受信。

#### タイムライン

| 時刻 | イベント | 対応者 |
|------|---------|--------|
| 14:30 | CloudWatchアラーム発火: エラー率急上昇 | 自動 |
| 14:32 | オンコール担当が確認開始 | 鈴木 |
| 14:35 | データベース接続数が上限に達していることを確認 | 鈴木 |
| 14:40 | 山田にエスカレーション、原因調査開始 | 山田 |
| 14:50 | 長時間実行クエリを特定・停止 | 山田 |
| 14:55 | 接続プールサイズを一時的に拡大 | 山田 |
| 15:00 | サービス復旧を確認 | 鈴木 |
| 15:15 | 完全復旧、監視継続 | 全員 |

#### 影響

**ユーザー影響**:
- 全ユーザーがサービスにアクセスできない状態
- API リクエストが500エラーを返す
- 進行中の操作（投稿作成など）が失敗

**ビジネス影響**:
- 新規登録: 約50件の機会損失
- 投稿作成: 約200件の失敗
- ユーザー体験の著しい低下

#### 根本原因

**直接原因**:
管理画面からの大規模データエクスポート処理が、長時間実行するクエリを発行し、データベース接続を占有した。

**根本原因**:
1. 接続プールサイズ（max: 20）が需要に対して不足
2. クエリタイムアウトが設定されていなかった
3. 大量データ処理が同期的に実行されていた

**技術的詳細**:
```sql
-- 問題のあったクエリ
SELECT * FROM posts 
  JOIN users ON posts.user_id = users.id
  JOIN comments ON posts.id = comments.post_id
WHERE created_at >= '2024-01-01'
ORDER BY created_at DESC;

-- 実行時間: 45秒
-- 取得行数: 500,000件
-- 接続占有時間: 45秒 × 15接続 = 675秒
```

#### 対応内容

**即時対応**（復旧作業）:
1. 長時間実行中のクエリを停止
```sql
SELECT pg_terminate_backend(pid) 
FROM pg_stat_activity 
WHERE state = 'active' 
  AND query_start < NOW() - INTERVAL '30 seconds';
```

2. 接続プールサイズを一時的に拡大
```typescript
// 20 → 50 に変更
DATABASE_MAX_CONNECTIONS=50
```

3. アプリケーションサーバーの再起動
```bash
aws ecs update-service --force-new-deployment
```

**恒久対策**（根本解決）:
実施済み:
- [x] クエリタイムアウトの設定（30秒）
- [x] 接続プールサイズの適正化（50接続）
- [x] 長時間実行クエリのアラート設定
- [x] 大量データエクスポートの非同期化

実施予定:
- [ ] クエリパフォーマンスの最適化（インデックス追加）
- [ ] 読み取り専用レプリカの導入
- [ ] 接続プール監視の強化

#### 再発防止策

**技術的対策**:

1. **クエリタイムアウトの設定**
```typescript
// database.config.ts
{
  connectionTimeoutMillis: 5000,
  query_timeout: 30000,  // 30秒
  statement_timeout: 30000
}
```

2. **接続プール監視**
```typescript
// 接続プール使用率を監視
pool.on('acquire', () => {
  const used = pool.totalCount - pool.idleCount;
  const usage = (used / pool.totalCount) * 100;
  
  if (usage > 80) {
    logger.warn(`Pool usage high: ${usage}%`);
  }
});
```

3. **大量データ処理の改善**
```typescript
// Before: 全データを一度に取得
const allData = await repository.findAll();

// After: ストリーム処理
const stream = repository.createQueryStream();
stream.on('data', (chunk) => processChunk(chunk));
```

**プロセス改善**:
- データエクスポートは非ピーク時（深夜）に実施
- 大規模クエリ実行前の影響評価を義務化
- 運用手順書の更新と訓練

**監視強化**:
- CloudWatch アラーム追加:
  - 接続プール使用率 > 80%
  - 長時間実行クエリ（> 10秒）
  - データベースレプリケーション遅延

#### ポストモーテム

**実施日**: 2025-11-10  
**参加者**: 開発チーム全員、インフラチーム

**良かった点**:
- アラートが迅速に発火
- 15分以内にエスカレーション
- チーム間の連携がスムーズ

**改善点**:
- 初動対応に時間がかかった（原因特定に20分）
- ユーザーへの告知が遅れた（発生から30分後）
- ロールバック手順が明確でなかった

**学び**:
- 接続プール監視の重要性
- クエリタイムアウトは必須
- 大量データ処理は別プロセスで

---

### INC-002: CDN障害によるスタイルシート未読み込み

| 項目 | 内容 |
|------|------|
| **インシデントID** | INC-002 |
| **発生日時** | 2025-11-05 09:15:00 |
| **検知日時** | 2025-11-05 09:20:00 |
| **復旧日時** | 2025-11-05 10:00:00 |
| **影響時間** | 45分 |
| **影響範囲** | フロントエンド（スタイル崩れ） |
| **影響ユーザー数** | 全ユーザー |
| **優先度** | P2 (High) |
| **ステータス** | 完了 |
| **担当者** | 田中次郎 |

#### 概要
CloudFrontのキャッシュ設定ミスにより、更新されたCSSファイルが配信されず、全ページでスタイルが崩れた。

#### 根本原因
- デプロイ時にCloudFrontのキャッシュ無効化を実行し忘れ
- CSSファイルのキャッシュ期間が7日間に設定されていた

#### 対応内容
1. CloudFrontのキャッシュを手動で無効化
2. デプロイスクリプトにキャッシュ無効化を追加

#### 再発防止策
```bash
# deploy.sh に追加
aws cloudfront create-invalidation \
  --distribution-id $DISTRIBUTION_ID \
  --paths "/*"
```

---

## 📊 障害統計

### 月次障害サマリー（2025年11月）

| 優先度 | 件数 | 平均復旧時間 | 総影響時間 |
|--------|------|-------------|-----------|
| P1 | 1 | 45分 | 45分 |
| P2 | 1 | 45分 | 45分 |
| P3 | 3 | 2時間 | 6時間 |
| P4 | 5 | 30分 | 2.5時間 |
| **合計** | **10** | **1.5時間** | **10時間** |

### カテゴリ別

| カテゴリ | 件数 | 割合 |
|---------|------|------|
| インフラ | 4 | 40% |
| アプリケーション | 3 | 30% |
| データベース | 2 | 20% |
| 外部サービス | 1 | 10% |

### 原因別

| 原因 | 件数 | 割合 |
|------|------|------|
| 設定ミス | 4 | 40% |
| キャパシティ不足 | 2 | 20% |
| バグ | 2 | 20% |
| 外部要因 | 2 | 20% |

## 📈 改善トレンド

### 月次比較

| 月 | 障害件数 | 平均MTTR | 可用性 |
|----|---------|---------|--------|
| 9月 | 15 | 3時間 | 99.2% |
| 10月 | 12 | 2時間 | 99.5% |
| 11月 | 10 | 1.5時間 | 99.7% |

**傾向**: 障害件数、復旧時間ともに改善傾向

## 📝 改善アクションアイテム

### 完了

- [x] クエリタイムアウト設定
- [x] 接続プール監視強化
- [x] デプロイスクリプト改善
- [x] アラート閾値の最適化

### 進行中

- [ ] 読み取りレプリカの導入（12月予定）
- [ ] 自動フェイルオーバー設定（12月予定）
- [ ] ディザスタリカバリ訓練（12月予定）

### 計画中

- [ ] カオスエンジニアリングの導入
- [ ] SLO/SLIの明確化
- [ ] オブザーバビリティ強化

## 📝 備考

### レポート作成ガイドライン

1. 障害発生後24時間以内に初期レポート作成
2. 復旧後7日以内にポストモーテム実施
3. 再発防止策は優先度付けて実施
4. 月次で障害傾向を分析

### 参考資料

- [Google - Postmortem Culture](https://sre.google/sre-book/postmortem-culture/)
- [Atlassian - Incident Management](https://www.atlassian.com/incident-management)

